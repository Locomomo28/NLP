{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1) Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK\n",
        "library. Use porter stemmer and snowball stemmer for stemming. Use any technique for\n",
        "lemmatization."
      ],
      "metadata": {
        "id": "fkxYGxSUwDnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoUxNucbk-Hf",
        "outputId": "ba05b914-f735-48ba-d0d5-99b89b0b230d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer\n",
        "from nltk.tokenize.mwe import MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")  # Download WordNet data for lemmatization\n",
        "\n",
        "# Sample Text\n",
        "text = \"Alice and Bob went to the enchanted forest, where they found a mysterious treasure chest! #AdventureTime\"\n",
        "\n",
        "# Tokenization Techniques\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "word_punct_tokenizer = WordPunctTokenizer()\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "# Multi-word expression tokenizer\n",
        "mwe_tokenizer = MWETokenizer([(\"Alice\", \"and\", \"Bob\"), (\"mysterious\", \"treasure\", \"chest\")])\n",
        "\n",
        "# Apply Tokenization\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
        "word_punct_tokens = word_punct_tokenizer.tokenize(text)\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "mwe_tokens = mwe_tokenizer.tokenize(treebank_tokens)  # Apply MWE on Treebank tokens\n",
        "\n",
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "porter_stems = [porter.stem(word) for word in treebank_tokens]\n",
        "snowball_stems = [snowball.stem(word) for word in treebank_tokens]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in treebank_tokens]\n",
        "\n",
        "# Display Results\n",
        "print(\"\\nTokenization\")\n",
        "print(\"Whitespace Tokenizer:\", whitespace_tokens)\n",
        "print(\"Punctuation-based Tokenizer:\", word_punct_tokens)\n",
        "print(\"Treebank Tokenizer:\", treebank_tokens)\n",
        "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
        "print(\"Multi-word Expression (MWE) Tokenizer:\", mwe_tokens)\n",
        "\n",
        "print(\"\\nStemming\")\n",
        "print(\"Porter Stemmer:\", porter_stems)\n",
        "print(\"Snowball Stemmer:\", snowball_stems)\n",
        "\n",
        "print(\"\\nLemmatization\")\n",
        "print(\"WordNet Lemmatizer:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDowKXwSlJKB",
        "outputId": "855eacc3-c7aa-423b-f689-2fdeffd1ac4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization\n",
            "Whitespace Tokenizer: ['Alice', 'and', 'Bob', 'went', 'to', 'the', 'enchanted', 'forest,', 'where', 'they', 'found', 'a', 'mysterious', 'treasure', 'chest!', '#AdventureTime']\n",
            "Punctuation-based Tokenizer: ['Alice', 'and', 'Bob', 'went', 'to', 'the', 'enchanted', 'forest', ',', 'where', 'they', 'found', 'a', 'mysterious', 'treasure', 'chest', '!', '#', 'AdventureTime']\n",
            "Treebank Tokenizer: ['Alice', 'and', 'Bob', 'went', 'to', 'the', 'enchanted', 'forest', ',', 'where', 'they', 'found', 'a', 'mysterious', 'treasure', 'chest', '!', '#', 'AdventureTime']\n",
            "Tweet Tokenizer: ['Alice', 'and', 'Bob', 'went', 'to', 'the', 'enchanted', 'forest', ',', 'where', 'they', 'found', 'a', 'mysterious', 'treasure', 'chest', '!', '#AdventureTime']\n",
            "Multi-word Expression (MWE) Tokenizer: ['Alice_and_Bob', 'went', 'to', 'the', 'enchanted', 'forest', ',', 'where', 'they', 'found', 'a', 'mysterious_treasure_chest', '!', '#', 'AdventureTime']\n",
            "\n",
            "Stemming\n",
            "Porter Stemmer: ['alic', 'and', 'bob', 'went', 'to', 'the', 'enchant', 'forest', ',', 'where', 'they', 'found', 'a', 'mysteri', 'treasur', 'chest', '!', '#', 'adventuretim']\n",
            "Snowball Stemmer: ['alic', 'and', 'bob', 'went', 'to', 'the', 'enchant', 'forest', ',', 'where', 'they', 'found', 'a', 'mysteri', 'treasur', 'chest', '!', '#', 'adventuretim']\n",
            "\n",
            "Lemmatization\n",
            "WordNet Lemmatizer: ['Alice', 'and', 'Bob', 'went', 'to', 'the', 'enchanted', 'forest', ',', 'where', 'they', 'found', 'a', 'mysterious', 'treasure', 'chest', '!', '#', 'AdventureTime']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}