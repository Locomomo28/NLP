{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q2) Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on\n",
        "data. Create embeddings using Word2Vec"
      ],
      "metadata": {
        "id": "wQQdCbg_xCVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data package\n",
        "nltk.download('punkt_tab')  # Correct resource\n",
        "\n",
        "documents = [\n",
        "    \"Alice and Bob discovered a hidden treasure in the old castle.\",\n",
        "    \"The treasure was buried under the ancient oak tree.\",\n",
        "    \"Legends spoke of a hidden treasure guarded by a mystical creature.\"\n",
        "]\n",
        "\n",
        "# Preprocess documents (e.g., converting to lowercase)\n",
        "processed_docs = [doc.lower() for doc in documents]\n",
        "\n",
        "# 1. Bag-of-Words (BoW)\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(processed_docs)\n",
        "bow_df = pd.DataFrame(bow_counts.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "print(\"=== Bag-of-Words Count Occurrence ===\")\n",
        "print(bow_df, \"\\n\")\n",
        "\n",
        "# 2. TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"=== TF-IDF Matrix ===\")\n",
        "print(tfidf_df, \"\\n\")\n",
        "\n",
        "# 3. Word2Vec Embeddings\n",
        "\n",
        "# Tokenize documents into words using NLTK\n",
        "tokenized_docs = [nltk.word_tokenize(doc) for doc in processed_docs]\n",
        "print(\"=== Tokenized Documents ===\")\n",
        "for i, tokens in enumerate(tokenized_docs, 1):\n",
        "    print(f\"Document {i}: {tokens}\")\n",
        "print()\n",
        "\n",
        "# Train a Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Display the vocabulary from the Word2Vec model\n",
        "vocab = list(w2v_model.wv.index_to_key)\n",
        "print(\"=== Vocabulary in Word2Vec Model ===\")\n",
        "print(vocab, \"\\n\")\n",
        "\n",
        "# Example: Get the embedding for the word \"treasure\"\n",
        "word = \"treasure\"\n",
        "if word in w2v_model.wv:\n",
        "    print(f\"=== Word2Vec Embedding for '{word}' ===\")\n",
        "    print(w2v_model.wv[word])\n",
        "else:\n",
        "    print(f\"Word '{word}' not found in the vocabulary.\")\n"
      ],
      "metadata": {
        "id": "My_Iq4RinKFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 gensim==4.3.1"
      ],
      "metadata": {
        "id": "gFM712XgQJck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scipy==1.10.1"
      ],
      "metadata": {
        "id": "F333lST5Qcf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}