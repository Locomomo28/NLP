{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lab Assignment 5: N-gram Language Model\n",
        "•\tImplement unigram, bigram, and trigram models using NLTK.\n",
        "•\tTrain on a small text dataset and compute probabilities of word sequences.\n",
        "•\tUse Laplace smoothing to handle unseen words."
      ],
      "metadata": {
        "id": "YufRAUSOxst0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnFv20v-xjaY"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install and import libraries\n",
        "!pip install -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Step 2: Sample small dataset (you can extend this or load from file)\n",
        "text = \"\"\"\n",
        "Natural language processing enables computers to understand human language.\n",
        "It involves linguistics and machine learning. Language models are essential in NLP.\n",
        "\"\"\"\n",
        "\n",
        "# Step 3: Tokenization and preprocessing\n",
        "tokens = word_tokenize(text.lower())\n",
        "tokens = [''] + tokens + ['']  # Start and end tokens\n",
        "\n",
        "# Step 4: Build Unigram, Bigram, and Trigram Models\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "vocab = set(tokens)\n",
        "V = len(vocab)  # Vocabulary size\n",
        "\n",
        "# Step 5: Define Probability Functions with Laplace Smoothing\n",
        "\n",
        "def unigram_prob(word):\n",
        "    return (unigram_counts[(word,)] + 1) / (sum(unigram_counts.values()) + V)\n",
        "\n",
        "def bigram_prob(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[(w1,)] + V)\n",
        "\n",
        "def trigram_prob(w1, w2, w3):\n",
        "    return (trigram_counts[(w1, w2, w3)] + 1) / (bigram_counts[(w1, w2)] + V)\n",
        "\n",
        "# Step 6: Compute Probabilities of Sample Sequences\n",
        "\n",
        "def compute_sequence_prob(sequence):\n",
        "    tokens = [''] + word_tokenize(sequence.lower()) + ['']\n",
        "\n",
        "    print(f\"\\nSequence: {sequence}\")\n",
        "\n",
        "    # Unigram\n",
        "    uni_prob = 1.0\n",
        "    for w in tokens:\n",
        "        prob = unigram_prob(w)\n",
        "        uni_prob *= prob\n",
        "    print(f\"Unigram Probability: {uni_prob:.10f} | LogProb: {math.log(uni_prob):.4f}\")\n",
        "\n",
        "    # Bigram\n",
        "    bi_prob = 1.0\n",
        "    for w1, w2 in ngrams(tokens, 2):\n",
        "        prob = bigram_prob(w1, w2)\n",
        "        bi_prob *= prob\n",
        "    print(f\"Bigram Probability: {bi_prob:.10f} | LogProb: {math.log(bi_prob):.4f}\")\n",
        "\n",
        "    # Trigram\n",
        "    tri_prob = 1.0\n",
        "    for w1, w2, w3 in ngrams(tokens, 3):\n",
        "        prob = trigram_prob(w1, w2, w3)\n",
        "        tri_prob *= prob\n",
        "    print(f\"Trigram Probability: {tri_prob:.10f} | LogProb: {math.log(tri_prob):.4f}\")\n",
        "\n",
        "# Step 7: Test the model on a sample input\n",
        "compute_sequence_prob(\"language models are essential\")\n",
        "compute_sequence_prob(\"computers learn language\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QutZjcQix3u_",
        "outputId": "9a181822-072d-4605-afcf-bbf0c4528bee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequence: language models are essential\n",
            "Unigram Probability: 0.0000000267 | LogProb: -17.4379\n",
            "Bigram Probability: 0.0000013611 | LogProb: -13.5072\n",
            "Trigram Probability: 0.0000178884 | LogProb: -10.9314\n",
            "\n",
            "Sequence: computers learn language\n",
            "Unigram Probability: 0.0000003139 | LogProb: -14.9741\n",
            "Bigram Probability: 0.0000039212 | LogProb: -12.4491\n",
            "Trigram Probability: 0.0001079797 | LogProb: -9.1336\n"
          ]
        }
      ]
    }
  ]
}