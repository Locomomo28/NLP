{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Lab Assignment 6: Word Embeddings with word2vec\n",
        "•\tTrain a word2vec model on a small corpus using Gensim.\n",
        "•\tVisualize word embeddings using t-SNE.\n",
        "•\tFind similar words using cosine similarity."
      ],
      "metadata": {
        "id": "WCipgh8KyeU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scipy==1.10.1 numpy==1.23.5 gensim==4.3.2\n",
        "! pip install scikit-learn nltk matplotlib"
      ],
      "metadata": {
        "id": "ba96jpceK3in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary modules\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "\n",
        "# Step 3: Sample text corpus (you can replace it with your own)\n",
        "corpus = \"\"\"\n",
        "Natural language processing enables machines to understand and interpret human language.\n",
        "It involves various tasks like sentiment analysis, named entity recognition, and machine translation.\n",
        "Deep learning has significantly improved NLP by using models like transformers and word embeddings.\n",
        "Word2Vec is an efficient model to learn vector representations of words based on context.\n",
        "\"\"\"\n",
        "\n",
        "# Step 4: Preprocess text\n",
        "nltk.download('punkt_tab')\n",
        "sentences = sent_tokenize(corpus.lower())\n",
        "tokenized_sentences = [word_tokenize(re.sub(r'[^\\w\\s]', '', sent)) for sent in sentences]\n",
        "\n",
        "# Step 5: Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=2, sg=1)  # sg=1 = skip-gram\n",
        "\n",
        "# Step 6: Explore vocabulary\n",
        "print(\"Vocabulary:\", list(model.wv.key_to_index.keys()))\n",
        "\n",
        "# Step 7: Find similar words\n",
        "word = 'language'\n",
        "print(f\"\\nWords similar to '{word}':\")\n",
        "print(model.wv.most_similar(word))\n",
        "\n",
        "# Step 8: Visualize embeddings using t-SNE\n",
        "import numpy as np\n",
        "def visualize_embeddings(model):\n",
        "    words = list(model.wv.key_to_index.keys())\n",
        "    word_vectors = [model.wv[word] for word in words]\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    reduced_vectors = np.array([model.wv[word] for word in words])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i, word in enumerate(words):\n",
        "        plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
        "        plt.annotate(word, xy=(reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
        "    plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_embeddings(model)"
      ],
      "metadata": {
        "id": "yzbfZNjlK9B-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}