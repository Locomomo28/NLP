{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1) Lab Assignment 1: Text Preprocessing and Regular Expressions\n",
        "Implement tokenization, stemming, and lemmatization using NLTK and spaCy.\n",
        "Use regular expressions for tasks such as extracting email addresses, phone numbers, and hashtags from a given text dataset of minimum 5 pages.\n",
        "     "
      ],
      "metadata": {
        "id": "-CwOvp95iB17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OElh414ah6fY"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 3: Simulate a sample 5-page dataset (you can load your own .txt or .csv here)\n",
        "text_data = \"\"\"\n",
        "Contact me at john.doe@example.com or jane_doe22@sample.org.\n",
        "My phone number is +1-800-555-1234 or (212) 555-4567.\n",
        "I love #MachineLearning and #AI!\n",
        "Barack Obama was the 44th president of the United States.\n",
        "SpaCy is great for NLP. NLTK is also useful.\n",
        "\n",
        "Email me at test.email@gmail.com or hello@mydomain.org.\n",
        "Call me at 987-654-3210 or 1234567890.\n",
        "Follow #Python and #DataScience on Twitter.\n",
        "The cat sat on the mat. The cats are sitting on the mats.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess into lines like different pages for simulation\n",
        "pages = text_data.strip().split('\\n')\n",
        "\n",
        "print(\"=== Tokenization ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    tokens = word_tokenize(page)\n",
        "    print(f\"\\nPage {i+1} Tokens:\\n\", tokens)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"\\n=== Stemming ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    tokens = word_tokenize(page)\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    print(f\"\\nPage {i+1} Stemmed:\\n\", stemmed)\n",
        "\n",
        "print(\"\\n=== Lemmatization (spaCy) ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    doc = nlp(page)\n",
        "    lemmatized = [token.lemma_ for token in doc]\n",
        "    print(f\"\\nPage {i+1} Lemmatized:\\n\", lemmatized)\n",
        "\n",
        "email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
        "phone_pattern = r'(\\+?\\d{1,3})?[\\s\\-\\.]??\\d2,4?[\\s\\-\\.]?\\d{3,4}[\\s\\-\\.]?\\d{4}'\n",
        "hashtag_pattern = r'#\\w+'\n",
        "\n",
        "print(\"\\n=== Regex Extraction ===\")\n",
        "for i, page in enumerate(pages):\n",
        "    emails = re.findall(email_pattern, page)\n",
        "    phones = re.findall(phone_pattern, page)\n",
        "    hashtags = re.findall(hashtag_pattern, page)\n",
        "\n",
        "    print(f\"\\nPage {i+1} Results:\")\n",
        "    print(\"Emails:\", emails)\n",
        "    print(\"Phone Numbers:\", phones)\n",
        "    print(\"Hashtags:\", hashtags)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B85Rzwk6q-bC",
        "outputId": "27c1fdc3-d649-4636-f4d2-2260b212a9ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tokenization ===\n",
            "\n",
            "Page 1 Tokens:\n",
            " ['Contact', 'me', 'at', 'john.doe', '@', 'example.com', 'or', 'jane_doe22', '@', 'sample.org', '.']\n",
            "\n",
            "Page 2 Tokens:\n",
            " ['My', 'phone', 'number', 'is', '+1-800-555-1234', 'or', '(', '212', ')', '555-4567', '.']\n",
            "\n",
            "Page 3 Tokens:\n",
            " ['I', 'love', '#', 'MachineLearning', 'and', '#', 'AI', '!']\n",
            "\n",
            "Page 4 Tokens:\n",
            " ['Barack', 'Obama', 'was', 'the', '44th', 'president', 'of', 'the', 'United', 'States', '.']\n",
            "\n",
            "Page 5 Tokens:\n",
            " ['SpaCy', 'is', 'great', 'for', 'NLP', '.', 'NLTK', 'is', 'also', 'useful', '.']\n",
            "\n",
            "Page 6 Tokens:\n",
            " []\n",
            "\n",
            "Page 7 Tokens:\n",
            " ['Email', 'me', 'at', 'test.email', '@', 'gmail.com', 'or', 'hello', '@', 'mydomain.org', '.']\n",
            "\n",
            "Page 8 Tokens:\n",
            " ['Call', 'me', 'at', '987-654-3210', 'or', '1234567890', '.']\n",
            "\n",
            "Page 9 Tokens:\n",
            " ['Follow', '#', 'Python', 'and', '#', 'DataScience', 'on', 'Twitter', '.']\n",
            "\n",
            "Page 10 Tokens:\n",
            " ['The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'The', 'cats', 'are', 'sitting', 'on', 'the', 'mats', '.']\n",
            "\n",
            "=== Stemming ===\n",
            "\n",
            "Page 1 Stemmed:\n",
            " ['contact', 'me', 'at', 'john.do', '@', 'example.com', 'or', 'jane_doe22', '@', 'sample.org', '.']\n",
            "\n",
            "Page 2 Stemmed:\n",
            " ['my', 'phone', 'number', 'is', '+1-800-555-1234', 'or', '(', '212', ')', '555-4567', '.']\n",
            "\n",
            "Page 3 Stemmed:\n",
            " ['i', 'love', '#', 'machinelearn', 'and', '#', 'ai', '!']\n",
            "\n",
            "Page 4 Stemmed:\n",
            " ['barack', 'obama', 'wa', 'the', '44th', 'presid', 'of', 'the', 'unit', 'state', '.']\n",
            "\n",
            "Page 5 Stemmed:\n",
            " ['spaci', 'is', 'great', 'for', 'nlp', '.', 'nltk', 'is', 'also', 'use', '.']\n",
            "\n",
            "Page 6 Stemmed:\n",
            " []\n",
            "\n",
            "Page 7 Stemmed:\n",
            " ['email', 'me', 'at', 'test.email', '@', 'gmail.com', 'or', 'hello', '@', 'mydomain.org', '.']\n",
            "\n",
            "Page 8 Stemmed:\n",
            " ['call', 'me', 'at', '987-654-3210', 'or', '1234567890', '.']\n",
            "\n",
            "Page 9 Stemmed:\n",
            " ['follow', '#', 'python', 'and', '#', 'datasci', 'on', 'twitter', '.']\n",
            "\n",
            "Page 10 Stemmed:\n",
            " ['the', 'cat', 'sat', 'on', 'the', 'mat', '.', 'the', 'cat', 'are', 'sit', 'on', 'the', 'mat', '.']\n",
            "\n",
            "=== Lemmatization (spaCy) ===\n",
            "\n",
            "Page 1 Lemmatized:\n",
            " ['contact', 'I', 'at', 'john.doe@example.com', 'or', 'jane_doe22@sample.org', '.']\n",
            "\n",
            "Page 2 Lemmatized:\n",
            " ['my', 'phone', 'number', 'be', '+1', '-', '800', '-', '555', '-', '1234', 'or', '(', '212', ')', '555', '-', '4567', '.']\n",
            "\n",
            "Page 3 Lemmatized:\n",
            " ['I', 'love', '#', 'MachineLearning', 'and', '#', 'AI', '!']\n",
            "\n",
            "Page 4 Lemmatized:\n",
            " ['Barack', 'Obama', 'be', 'the', '44th', 'president', 'of', 'the', 'United', 'States', '.']\n",
            "\n",
            "Page 5 Lemmatized:\n",
            " ['SpaCy', 'be', 'great', 'for', 'NLP', '.', 'NLTK', 'be', 'also', 'useful', '.']\n",
            "\n",
            "Page 6 Lemmatized:\n",
            " []\n",
            "\n",
            "Page 7 Lemmatized:\n",
            " ['email', 'I', 'at', 'test.email@gmail.com', 'or', 'hello@mydomain.org', '.']\n",
            "\n",
            "Page 8 Lemmatized:\n",
            " ['call', 'I', 'at', '987', '-', '654', '-', '3210', 'or', '1234567890', '.']\n",
            "\n",
            "Page 9 Lemmatized:\n",
            " ['follow', '#', 'Python', 'and', '#', 'DataScience', 'on', 'Twitter', '.']\n",
            "\n",
            "Page 10 Lemmatized:\n",
            " ['the', 'cat', 'sit', 'on', 'the', 'mat', '.', 'the', 'cat', 'be', 'sit', 'on', 'the', 'mat', '.']\n",
            "\n",
            "=== Regex Extraction ===\n",
            "\n",
            "Page 1 Results:\n",
            "Emails: ['john.doe@example.com', 'jane_doe22@sample.org.']\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 2 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 3 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: ['#MachineLearning', '#AI']\n",
            "\n",
            "Page 4 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 5 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 6 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 7 Results:\n",
            "Emails: ['test.email@gmail.com', 'hello@mydomain.org.']\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 8 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n",
            "\n",
            "Page 9 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: ['#Python', '#DataScience']\n",
            "\n",
            "Page 10 Results:\n",
            "Emails: []\n",
            "Phone Numbers: []\n",
            "Hashtags: []\n"
          ]
        }
      ]
    }
  ]
}
